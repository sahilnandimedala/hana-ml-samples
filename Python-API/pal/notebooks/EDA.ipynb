{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "**Arun Godwin Patel**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an essential tool in the Data Science toolbox. It is the process of understanding your dataset using statistical techniques and visualizations. The insight that you gain from EDA can help you to uncover issues, dra\n",
    "\n",
    "## Titanic Dataset\n",
    "This dataset is from https://github.com/awesomedata/awesome-public-datasets/tree/master/Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml import dataframe\n",
    "from hana_ml.algorithms.pal import trees\n",
    "from hana_ml.visualizers.eda import EDAVisualizer as eda\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hana_ml.visualizers.eda import EDAVisualizer, hist, kdeplot\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00.000.00.1683631277 (fa/CE2023.4)\n",
      "ML_USER\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(os.getcwd(),'../../config/HANA_ENV.json')) as f:\n",
    "    hana_env_c = json.load(f)\n",
    "    url_c = hana_env_c['url']\n",
    "    port_c = hana_env_c['port']\n",
    "    user_c = hana_env_c['user']\n",
    "    pwd_c = hana_env_c['pwd']\n",
    "\n",
    "cc = dataframe.ConnectionContext(url_c,port_c,user_c,pwd_c)\n",
    "print(cc.hana_version())\n",
    "print(cc.get_current_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "ERROR:hana_ml.algorithms.pal.partition:(2048, 'column store error: \"ML_USER\".\"(DO statement)\": line 29 col 1 (at pos 1104): search table error:  [34091] No ScriptServer available. See SAPNote 1650957 for further information.')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mm0450.MAVEN\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\partition.py\", line 236, in train_test_val_split\n",
      "    call_pal_auto_with_hint(conn,\n",
      "  File \"C:\\Users\\mm0450.MAVEN\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\pal_base.py\", line 1363, in call_pal_auto_with_hint\n",
      "    if try_exec(cur, sql, conn):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mm0450.MAVEN\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\pal_base.py\", line 1322, in try_exec\n",
      "    cur.execute(sql)\n",
      "hdbcli.dbapi.Error: (2048, 'column store error: \"ML_USER\".\"(DO statement)\": line 29 col 1 (at pos 1104): search table error:  [34091] No ScriptServer available. See SAPNote 1650957 for further information.')\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "(2048, 'column store error: \"ML_USER\".\"(DO statement)\": line 29 col 1 (at pos 1104): search table error:  [34091] No ScriptServer available. See SAPNote 1650957 for further information.')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhana_ml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutility\u001b[39;00m \u001b[39mimport\u001b[39;00m DataSets, Settings\n\u001b[0;32m      2\u001b[0m \u001b[39m# url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# cc = dataframe.ConnectionContext(url, port, user, pwd)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m full_set, training_set, validation_set, test_set \u001b[39m=\u001b[39m DataSets\u001b[39m.\u001b[39;49mload_titanic_data(cc, force\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, chunk_size\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\utility.py:377\u001b[0m, in \u001b[0;36mDataSets.load_titanic_data\u001b[1;34m(connection, schema, chunk_size, force, train_percentage, valid_percentage, test_percentage, full_tbl, seed, url)\u001b[0m\n\u001b[0;32m    357\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mparsers\u001b[39m.\u001b[39mread_csv(url,\n\u001b[0;32m    358\u001b[0m                                   header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m                                   names\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mPASSENGER_ID\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m                                          \u001b[39m'\u001b[39m\u001b[39mEMBARKED\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    370\u001b[0m                                          \u001b[39m'\u001b[39m\u001b[39mSURVIVED\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    371\u001b[0m     full_df \u001b[39m=\u001b[39m create_dataframe_from_pandas(connection,\n\u001b[0;32m    372\u001b[0m                                            pandas_df\u001b[39m=\u001b[39mdata,\n\u001b[0;32m    373\u001b[0m                                            table_name\u001b[39m=\u001b[39mfull_tbl,\n\u001b[0;32m    374\u001b[0m                                            schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m    375\u001b[0m                                            force\u001b[39m=\u001b[39mforce,\n\u001b[0;32m    376\u001b[0m                                            chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m--> 377\u001b[0m train_df, test_df, valid_df \u001b[39m=\u001b[39m train_test_val_split(full_df,\n\u001b[0;32m    378\u001b[0m                                                    id_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPASSENGER_ID\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    379\u001b[0m                                                    random_seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    380\u001b[0m                                                    partition_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrandom\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    381\u001b[0m                                                    training_percentage\u001b[39m=\u001b[39;49mtrain_percentage,\n\u001b[0;32m    382\u001b[0m                                                    testing_percentage\u001b[39m=\u001b[39;49mtest_percentage,\n\u001b[0;32m    383\u001b[0m                                                    validation_percentage\u001b[39m=\u001b[39;49mvalid_percentage)\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m full_df, train_df, valid_df, test_df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\partition.py:236\u001b[0m, in \u001b[0;36mtrain_test_val_split\u001b[1;34m(data, id_column, random_seed, thread_ratio, partition_method, stratified_column, training_percentage, testing_percentage, validation_percentage, training_size, testing_size, validation_size)\u001b[0m\n\u001b[0;32m    233\u001b[0m     param_rows\u001b[39m.\u001b[39mextend([(\u001b[39m'\u001b[39m\u001b[39mSTRATIFIED_COLUMN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, stratified_column)])\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     call_pal_auto_with_hint(conn,\n\u001b[0;32m    237\u001b[0m                             \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    238\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mPAL_PARTITION\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    239\u001b[0m                             data,\n\u001b[0;32m    240\u001b[0m                             ParameterTable()\u001b[39m.\u001b[39;49mwith_data(param_rows),\n\u001b[0;32m    241\u001b[0m                             result_tbl)\n\u001b[0;32m    242\u001b[0m \u001b[39mexcept\u001b[39;00m dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m db_err:\n\u001b[0;32m    243\u001b[0m     logger\u001b[39m.\u001b[39mexception(\u001b[39mstr\u001b[39m(db_err))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\pal_base.py:1363\u001b[0m, in \u001b[0;36mcall_pal_auto_with_hint\u001b[1;34m(conn, with_hint, funcname, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[39m# Optimistic execution.\u001b[39;00m\n\u001b[0;32m   1362\u001b[0m \u001b[39mwith\u001b[39;00m conn\u001b[39m.\u001b[39mconnection\u001b[39m.\u001b[39mcursor() \u001b[39mas\u001b[39;00m cur:\n\u001b[1;32m-> 1363\u001b[0m     \u001b[39mif\u001b[39;00m try_exec(cur, sql, conn):\n\u001b[0;32m   1364\u001b[0m         \u001b[39m# Optimistic execution succeeded, meaning all arguments with\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m         \u001b[39m# unknown ttab safety are safe.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m unknown_indices:\n\u001b[0;32m   1367\u001b[0m             adjusted_args[i]\u001b[39m.\u001b[39mdeclare_lttab_usage(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\hana_ml\\algorithms\\pal\\pal_base.py:1322\u001b[0m, in \u001b[0;36mcall_pal_auto_with_hint.<locals>.try_exec\u001b[1;34m(cur, sql, conn)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1321\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1322\u001b[0m         cur\u001b[39m.\u001b[39;49mexecute(sql)\n\u001b[0;32m   1323\u001b[0m     \u001b[39mexcept\u001b[39;00m dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1324\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39merrortext\u001b[39m.\u001b[39mstartswith(\n\u001b[0;32m   1325\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mfeature not supported: Cannot use local temporary table\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mError\u001b[0m: (2048, 'column store error: \"ML_USER\".\"(DO statement)\": line 29 col 1 (at pos 1104): search table error:  [34091] No ScriptServer available. See SAPNote 1650957 for further information.')"
     ]
    }
   ],
   "source": [
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\n",
    "# url, port, user, pwd = Settings.load_config(\"../../config/e2edata.ini\")\n",
    "# cc = dataframe.ConnectionContext(url, port, user, pwd)\n",
    "full_set, training_set, validation_set, test_set = DataSets.load_titanic_data(cc, force=False, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HANA Dataframe (df_train) and point to the training table.\n",
    "data = full_set\n",
    "data.head(5).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset we have 12 columns:\n",
    "- **PassengerId** - Unique ID assigned to each passenger.\n",
    "- **Pclass** - Class of ticket purchased (1 = 1st class, 2 = 2nd class, 3 = 3rd class).\n",
    "- **Name** - Full name and title of the passenger.\n",
    "- **Sex** - Gender of the passenger.\n",
    "- **Age** - The Age of the passenger in years.\n",
    "- **SibSp** - Number of siblings and spouses associated with the passenger aboard.\n",
    "- **Parch** - Number of parents and children associated with the passenger aboard.\n",
    "- **Ticket** - Ticket number.\n",
    "- **Fare** - The fare of the ticket purchased by the passenger.\n",
    "- **Cabin** - The Cabin number that the passenger was assigned to. If NaN, this means they had no cabin and perhaps were not assigned one due to the cost of their ticket.\n",
    "- **Embarked** - Port of embarkation (S = Southampton, C = Cherbourg, Q = Queenstown).\n",
    "- **Survived** - Survival flag of passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This description is useful to first understand some basic statistics over a dataset. We are presented with:\n",
    "- **Column**: Name of the column\n",
    "- **Count**: Count of rows for this column\n",
    "- **Unique**: Distinct count of this column\n",
    "- **Nulls**: Total rows with null values for this column\n",
    "- **Mean**: Sum of the column divided by the count\n",
    "- **Std**: Standard deviation, a quantity expressing by how much the members of a group differ from the mean value for the group\n",
    "- **Min**: Minimum value of the column\n",
    "- **Max**: Maximum value of the column\n",
    "- **Median**: The 50th percentile value (percentiles explained below)\n",
    "- **25, 50 & 75 percentiles**: A percentile is the value that represents a division of the ordered dataset by a certain %. For example, the 10th percentile is the value that when the dataset is ordered, splits the bottom 10% from the remaining 90%. e.g. you have numbers:\n",
    "\n",
    "### 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20\n",
    "\n",
    "**What is the 10th percentile? The dataset is already ordered, now we must simply split the data after 10% of the values.**\n",
    "\n",
    "### 1 2   |   3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
    "\n",
    "Hence, it equals 2.\n",
    "\n",
    "Percentiles are useful to understand the lower (25%), middle (50% AKA median) and upper (75%) quartiles. This is useful when calculating the inter-quartile range, which is a way of understanding the spread (dispersion) of values within data. It is calculated by the difference between the upper and lower quartile. The IQR also gives you an indication of the spread of the middle 50% of values within your column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plot\n",
    "\n",
    "- Displaying the distribution shows us how the values of a column are spread in regards to frequency.\n",
    "- By looking at the x axis bins, we can see the range of values that occured for the column.\n",
    "- Looking at a distribution can give you an indication on how skewed your column is from the normal distribution. This is important to know because highly positive or negatively skewed data can cause a predictive model to overfit to extreme values.\n",
    "- Extreme values are large values that could be considered as outliers, and may skew results of a Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(35, 10))\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, dist_data = eda.distribution_plot(data=data, column=\"FARE\", bins=100, title=\"Distribution of FARE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plot for AGE between survivors and non-survivors\n",
    "\n",
    "- The distributions of age for survivors and non-survivors follow a very similar distribution. \n",
    "- This tells us that the column 'AGE' may not be a highly significant factor in predicting survival, because it follows a very similar pattern for survivors/non-survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(18, 6))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, dist_data = eda.distribution_plot(data=data.filter(\"SURVIVED = 0\"), column=\"AGE\", bins=20, title=\"Distribution of AGE for non-survivors\")\n",
    "\n",
    "ax1 = f.add_subplot(122)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, dist_data = eda.distribution_plot(data=data.filter(\"SURVIVED = 1\"), column=\"AGE\", bins=20, title=\"Distribution of AGE for survivors\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(19, 10))\n",
    "\n",
    "ax = kdeplot(data, key=\"PASSENGER_ID\", features=[\"AGE\"])\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(19, 10))\n",
    "\n",
    "ax, surf = kdeplot(data, key=\"PASSENGER_ID\", features=[\"AGE\", \"FARE\"])\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie plot\n",
    "\n",
    "- A pie plot shows us the distribution of values between categories as a percentage.\n",
    "- These are particulalrly useful for categorical columns.\n",
    "- Each coloured portion of the pie plot is labelled to represent a different value within the column.\n",
    "- The size of the coloured portion is proportional to the percentage of occurences this value takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, pie_data = eda.pie_plot(data, column=\"PCLASS\", title=\"% of passengers in each cabin\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie plot for PCLASS distribution between survivors and non-survivors\n",
    "\n",
    "- We can see that the percentage of survivors and non-survivors varied quite largely between cabins, for example the percentage of non-survivors in the 1st class cabin was 14.4% but the percentage of survivors that were in 1st class was 44.4%.\n",
    "- It shows us that the largest majority of non-survivors came from 3rd class, and the largest majority of survivors came from 1st class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, pie_data = eda.pie_plot(data=data.filter(\"SURVIVED = 0\"), column=\"PCLASS\", title=\"% of non-survivors in each cabin\")\n",
    "ax1.legend(loc='best') # User has control over axis aesthetics\n",
    "\n",
    "ax1 = f.add_subplot(122)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, pie_data = eda.pie_plot(data=data.filter(\"SURVIVED = 1\"), column=\"PCLASS\", title=\"% of survivors in each cabin\")\n",
    "ax1.legend(loc='best') # User has control over axis aesthetics\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plot\n",
    "\n",
    "- A nice way to understand the linear relationship between multiple variables is by using a correlation matrix.\n",
    "- The values shown in the matrix range from -1 to 1 and are calculations of Pearson's correlation coefficient (r). The 'r' value tells us the extent to which the variance of 2 columns follow a linear relationship. For example:\n",
    "    - If r = -1 for FARE and AGE, this tells me that as FARE increases, AGE proportionally decreases.\n",
    "    - If r = 1 for FARE and AGE, this tells me that as FARE increases, AGE also proportionally increases.\n",
    "    - A correlation of 1 or -1 would be very suspicious and would indicate that these columns may contain similar information.\n",
    "    - If r = 0 for FARE and AGE, this tells me that FARE and AGE have no linear relationship and are in fact, independant.\n",
    "- We can see here that 'PARCH' and 'SIBSP' are the highest correlating numeric features. With Machine Learning, we ideally want a set of independant variables, highly correlating variables can cause multicolinearity, which causes skewed results due to leaker variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, corr = eda.correlation_plot(data=data, corr_cols=['PCLASS', 'AGE', 'SIBSP', 'PARCH', 'FARE'], cmap=\"Blues\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data=data, columns=['PCLASS', 'AGE', 'SIBSP', 'PARCH', 'FARE'], default_bins=20, bins={\"AGE\": 10})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, corr = eda.scatter_plot(data=data, x=\"AGE\", y=\"SIBSP\", x_bins=5, y_bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "ax2 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax2)\n",
    "ax2 = eda.scatter_plot(data=data, x=\"AGE\", y=\"SIBSP\", sample_frac=0.8, s=10, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, corr = eda.box_plot(data=data, column=\"AGE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, corr = eda.box_plot(data=data, column=\"AGE\", groupby=\"SEX\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection to SAP HANA\n",
    "connection_context.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8e26eb492012ce43ec3ea98c3fc2503d5495ecd40107dd94395e1e0d860e85"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
